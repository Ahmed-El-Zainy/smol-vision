{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf39de21",
   "metadata": {},
   "source": [
    "# üöÄ Multi-Vector Image Retrieval Optimization\n",
    "\n",
    "Welcome! This is an **open-ended project** where you'll use **Jupyter AI** to implement vector optimization techniques. Jupyter AI is an AI-powered coding assistant built directly into JupyterLab - simply describe what you want to build in the chat panel, and it generates working code for you to run.\n",
    "\n",
    "---\n",
    "\n",
    "**Goal:** Optimize multi-vector search using **Scalar Quantization** with optional **HNSW** for speed\n",
    "\n",
    "> ### üéØ What You'll Achieve:\n",
    "> **75% memory reduction** + ‚ö° **Faster, more efficient search** - with **zero accuracy loss!**\n",
    "\n",
    "**What's pre-built:** Baseline ColPali search with 5 paper pages already indexed\n",
    "\n",
    "**Your task:** Implement scalar quantization and see the memory/speed trade-offs!\n",
    "\n",
    "---\n",
    "\n",
    "![Jupyter AI Chat](images/jupyter_chat_bordered.png)\n",
    "\n",
    "> üí° **New to JupyterAI?** Learn more at [JupyterAI: Coding in Notebooks](https://www.deeplearning.ai/short-courses/jupyter-ai-coding-in-notebooks/)\n",
    "\n",
    "> ‚ö†Ô∏è **Session expires in 2 hours** - download `project.ipynb` regularly!\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° IMPORTANT: Attach Files to Jupyter AI\n",
    "\n",
    "**Before asking Jupyter AI for help, attach these files to your prompts:**\n",
    "- `project.ipynb` - Your code and progress\n",
    "- `spec.md` - Optimization challenge details  \n",
    "- `docs.md` - Qdrant quantization API reference\n",
    "\n",
    "This gives Jupyter AI the context it needs to generate correct code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b27d8d",
   "metadata": {},
   "source": [
    "## Setup: Load Baseline\n",
    "\n",
    "Run the cell below to:\n",
    "1. Load 5 pre-computed ColPali embeddings from \"Attention is All You Need\" paper\n",
    "2. Create Qdrant collection and index images\n",
    "3. Load ColPali model\n",
    "4. Run baseline search for: `\"transformer architecture diagram\"`\n",
    "\n",
    "You'll see results immediately!\n",
    "\n",
    "> üí° **Want to verify the search results?** All page images are available in `attention_paper/` folder (page-0.png through page-9.png). Check them to see if you agree with the ranking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30331d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading pre-computed embeddings...\n",
      "‚úÖ Loaded 5 images\n",
      "   Total vectors: 5120\n",
      "   Vector dimension: 128\n",
      "   Memory (float32): 2.50 MB\n",
      "\n",
      "üóÑÔ∏è  Creating baseline collection...\n",
      "‚úÖ Indexed 5 images\n",
      "\n",
      "ü§ñ Loading ColPali model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a87ce41400400c98acd45223c7c410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/489 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa60e9a6a72b4af090b830f4c3a81f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c45ef2057804965be4f815ca35618f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from baseline_helper import BaselineSetup\n",
    "\n",
    "# Run complete baseline setup\n",
    "baseline = BaselineSetup()\n",
    "query_embedding, baseline_metrics = baseline.setup_all()\n",
    "\n",
    "print(\"\\n‚úÖ Baseline ready! Now choose your optimization below...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c1e6c",
   "metadata": {},
   "source": [
    "## üéØ Your Task: Implement Scalar Quantization\n",
    "\n",
    "**Choose one approach:**\n",
    "- **Option 1: Memory Optimization** (Scalar Quantization only)\n",
    "- **Option 2: Memory + Speed** (Scalar Quantization + HNSW)\n",
    "\n",
    "---\n",
    "\n",
    "### Example Prompts for Jupyter AI\n",
    "\n",
    "Copy ONE of these prompts to the Jupyter AI chatbot:\n",
    "\n",
    "<details>\n",
    "<summary><strong>Option 1: Scalar Quantization Only (Recommended - Start Here)</strong></summary>\n",
    "\n",
    "```\n",
    "Implement scalar quantization optimization for ColPali multi-vector search:\n",
    "\n",
    "1. Import necessary Qdrant models (check docs.md for ScalarQuantization imports)\n",
    "2. Create a new collection called \"optimized\" with scalar quantization enabled (INT8)\n",
    "3. Use the same multi-vector configuration as baseline (MaxSim)\n",
    "4. Disable HNSW for exact search: hnsw_config=HnswConfigDiff(m=0)\n",
    "5. Index all 5 images from baseline.embeddings_df using their original embeddings\n",
    "6. Run the same query as baseline using baseline.processor and baseline.model\n",
    "7. Measure search time and store results\n",
    "8. Calculate memory metrics (quantized uses 1 byte per dim vs baseline's 4 bytes)\n",
    "9. Use print_comparison() from baseline_helper to show results\n",
    "\n",
    "Expected results:\n",
    "- 75% memory reduction (2.50 MB ‚Üí 0.62 MB)\n",
    "- Same or better accuracy (exact search)\n",
    "- Similar speed (brute-force MaxSim)\n",
    "\n",
    "Remember: Qdrant handles quantization internally, just send original float32 embeddings!\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Option 2: Scalar Quantization + HNSW (Advanced - Try Second)</strong></summary>\n",
    "\n",
    "```\n",
    "Implement scalar quantization with HNSW for combined memory + speed optimization:\n",
    "\n",
    "1. Import necessary Qdrant models (check docs.md for ScalarQuantization imports)\n",
    "2. Create a new collection called \"optimized_hnsw\" with scalar quantization enabled (INT8)\n",
    "3. Use the same multi-vector configuration as baseline (MaxSim)\n",
    "4. Enable HNSW for approximate search: hnsw_config=HnswConfigDiff(m=16, ef_construct=100)\n",
    "5. Index all 5 images from baseline.embeddings_df using their original embeddings\n",
    "6. Run the same query as baseline using baseline.processor and baseline.model\n",
    "7. Measure search time and store results\n",
    "8. Calculate memory metrics (same as scalar-only: 1 byte per dim)\n",
    "9. Use print_comparison() from baseline_helper to show results\n",
    "\n",
    "Expected results:\n",
    "- 75% memory reduction (same as scalar-only)\n",
    "- Potentially faster on large datasets (note: 5 images is too small to see HNSW benefits)\n",
    "- Slightly lower accuracy (approximate search trade-off)\n",
    "\n",
    "Key insight: HNSW shows speed benefits on 1000+ vectors, not 5 images!\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**After copying a prompt:** Paste it in Jupyter AI chat, then paste the generated code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ IMPLEMENT YOUR OPTIMIZATION HERE\n",
    "# Paste the code generated by Jupyter AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3c910",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the multi-vector optimization challenge!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- **Scalar Quantization**: 75% memory reduction by compressing float32 ‚Üí int8\n",
    "- **Trade-offs**: Memory vs Speed vs Accuracy in vector search\n",
    "- **HNSW**: Approximate search for speed (benefits appear on large datasets)\n",
    "- **Stacking optimizations**: Quantization + HNSW for memory AND speed\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Option 1 (Scalar Only):**\n",
    "- ‚úÖ 75% memory reduction\n",
    "- ‚úÖ Perfect accuracy (exact search)\n",
    "- ‚úÖ Simple implementation\n",
    "- Best for: Memory-constrained environments\n",
    "\n",
    "**Option 2 (Scalar + HNSW):**\n",
    "- ‚úÖ 75% memory reduction (same)\n",
    "- ‚ö° Faster on large datasets (1000+ vectors)\n",
    "- ‚ö†Ô∏è Slight accuracy trade-off (approximate search)\n",
    "- Best for: Large-scale production systems\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Compare both approaches**: Try both Option 1 and Option 2 to see the HNSW parameter difference\n",
    "2. **Production tip**: Start with Scalar-only, add HNSW when scaling to 1000+ documents\n",
    "\n",
    "### üî¨ Extensions to Try (After Session)\n",
    "\n",
    "<details>\n",
    "<summary><strong>Experiment with Different Queries</strong></summary>\n",
    "\n",
    "Test how ColPali handles different semantic concepts:\n",
    "\n",
    "```python\n",
    "# In Cell 2, modify the query:\n",
    "query_embedding, baseline_metrics = baseline.setup_all(\n",
    "    query=\"attention mechanism\"  # Try different queries!\n",
    ")\n",
    "```\n",
    "\n",
    "**Try these queries:**\n",
    "- `\"attention mechanism\"` - Focus on specific component\n",
    "- `\"encoder decoder stacks\"` - More specific architecture detail\n",
    "- `\"multi head attention\"` - Another key component\n",
    "- `\"figure 1\"` - Meta query (searching by reference)\n",
    "- `\"model architecture\"` - More general query\n",
    "\n",
    "**What to observe:**\n",
    "- Which pages rank highest for each query?\n",
    "- Do text-heavy pages rank higher than diagram pages?\n",
    "- How does quantization affect accuracy for different queries?\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Use More Pages from the Paper</strong></summary>\n",
    "\n",
    "Index all 10 available pages instead of just 5:\n",
    "\n",
    "```python\n",
    "# In Cell 2, modify to use all 10 pages:\n",
    "query_embedding, baseline_metrics = baseline.setup_all(\n",
    "    available_pages=list(range(10))  # All 10 pages instead of default [0,1,2,3,4]\n",
    ")\n",
    "```\n",
    "\n",
    "**Expected changes:**\n",
    "- Memory: 2.50 MB ‚Üí 5.00 MB (baseline)\n",
    "- Memory: 0.62 MB ‚Üí 1.25 MB (optimized)\n",
    "- Still 75% reduction!\n",
    "- More diverse search results\n",
    "- HNSW benefits become slightly more noticeable\n",
    "\n",
    "**What to observe:**\n",
    "- Does the ranking change with more pages?\n",
    "- How does search time scale with 2x the vectors?\n",
    "- Is quantization accuracy still perfect?\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Package as a Python Script\n",
    "\n",
    "Want to turn your notebook into a reusable `.py` script? **Jupyter AI can help!** Ask it to:\n",
    "- Extract your optimization code from Cell 4\n",
    "- Package it as a standalone Python script\n",
    "- Add command-line arguments for different queries and page ranges\n",
    "\n",
    "This makes your optimization code production-ready!\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Optional Feedback Survey ¬∂\n",
    "\n",
    "We'd love to hear about your experience! Your feedback helps us create more valuable educational experiences.\n",
    "\n",
    "**[Take the short survey ‚Üí](https://rebrand.ly/xudvvw2)**\n",
    "\n",
    "This optional survey asks about:\n",
    "\n",
    "- Project quality and engagement\n",
    "- What you found most valuable\n",
    "- How we can improve future projects\n",
    "\n",
    "Thank you for your time! üôè\n",
    "\n",
    "---\n",
    "\n",
    "> ‚ö†Ô∏è **Session expires in 2 hours** - download your completed `project.ipynb` now!\n",
    "\n",
    "> üìö **Learn more**: Check out the full [Qdrant Optimization course](https://www.deeplearning.ai/short-courses/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
