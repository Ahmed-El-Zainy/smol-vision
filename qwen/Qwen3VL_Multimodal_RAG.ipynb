{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Doz8nKeWU4Q"
      },
      "source": [
        "## Multimodal RAG Example using Qwen3-VL Models\n",
        "\n",
        "This example demonstrates a complete multimodal RAG pipeline that can:\n",
        "\n",
        "- Process PDF documents into images\n",
        "- Create embeddings for both text queries and document images to use to search and retrieve relevant pages\n",
        "- Rerank results for better accuracy\n",
        "- Generate answers using Qwen3-VL\n",
        "\n",
        "**Note:** These models are large. Make sure you have sufficient GPU memory. You need to download the model repositories that include the scripts folder. You can install flash attention if your GPU allows, and quantize the models using bitsandbytes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VE3jgbfnPQjs"
      },
      "outputs": [],
      "source": [
        "!pip install -q pdf2image>=1.16.0 qwen-vl-utils flash-attn bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgV8BebePbl-",
        "outputId": "35795eae-9b7b-4288-bab7-8b68a18263dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Fetched 186 kB in 1s (188 kB/s)         \n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQbwhYg8WNnb"
      },
      "source": [
        "Download scripts that required to wrap Qwen3-VL models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nuKol8KU4j8",
        "outputId": "33e2c24b-597d-4c16-af32-b2708d3651cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-10 17:14:47--  https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/resolve/main/scripts/qwen3_vl_embedding.py\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /api/resolve-cache/models/Qwen/Qwen3-VL-Embedding-8B/a12d6118f720ceb6d95f7d1cad4e8aeccddd9340/scripts%2Fqwen3_vl_embedding.py?%2FQwen%2FQwen3-VL-Embedding-8B%2Fresolve%2Fmain%2Fscripts%2Fqwen3_vl_embedding.py=&etag=%2236d45865735be96a1278a21c132ff640e2ae68ca%22 [following]\n",
            "--2026-01-10 17:14:47--  https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-VL-Embedding-8B/a12d6118f720ceb6d95f7d1cad4e8aeccddd9340/scripts%2Fqwen3_vl_embedding.py?%2FQwen%2FQwen3-VL-Embedding-8B%2Fresolve%2Fmain%2Fscripts%2Fqwen3_vl_embedding.py=&etag=%2236d45865735be96a1278a21c132ff640e2ae68ca%22\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13258 (13K) [text/plain]\n",
            "Saving to: ‘qwen3_vl_embedding.py’\n",
            "\n",
            "qwen3_vl_embedding. 100%[===================>]  12.95K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-01-10 17:14:47 (420 MB/s) - ‘qwen3_vl_embedding.py’ saved [13258/13258]\n",
            "\n",
            "--2026-01-10 17:14:47--  https://huggingface.co/Qwen/Qwen3-VL-Reranker-8B/resolve/main/scripts/qwen3_vl_reranker.py\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /api/resolve-cache/models/Qwen/Qwen3-VL-Reranker-8B/8e52ab8fdc69d698b3e1c17f9977afb6fea4f656/scripts%2Fqwen3_vl_reranker.py?%2FQwen%2FQwen3-VL-Reranker-8B%2Fresolve%2Fmain%2Fscripts%2Fqwen3_vl_reranker.py=&etag=%22db7c53f9ad0971a41065053be5ebc432a978a993%22 [following]\n",
            "--2026-01-10 17:14:47--  https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-VL-Reranker-8B/8e52ab8fdc69d698b3e1c17f9977afb6fea4f656/scripts%2Fqwen3_vl_reranker.py?%2FQwen%2FQwen3-VL-Reranker-8B%2Fresolve%2Fmain%2Fscripts%2Fqwen3_vl_reranker.py=&etag=%22db7c53f9ad0971a41065053be5ebc432a978a993%22\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10873 (11K) [text/plain]\n",
            "Saving to: ‘qwen3_vl_reranker.py’\n",
            "\n",
            "qwen3_vl_reranker.p 100%[===================>]  10.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-01-10 17:14:47 (323 MB/s) - ‘qwen3_vl_reranker.py’ saved [10873/10873]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/resolve/main/scripts/qwen3_vl_embedding.py\n",
        "!wget https://huggingface.co/Qwen/Qwen3-VL-Reranker-8B/resolve/main/scripts/qwen3_vl_reranker.py\n",
        "!mkdir scripts\n",
        "!mv qwen3_vl_embedding.py scripts/\n",
        "!mv qwen3_vl_reranker.py scripts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6SmEpvZcemw"
      },
      "source": [
        "Some utils to convert PDF to images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xhu1f65TUfx4"
      },
      "outputs": [],
      "source": [
        "from pdf2image import convert_from_path\n",
        "import requests\n",
        "\n",
        "def download_pdf(url, save_path=\"document.pdf\"):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"PDF saved to {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def pdf_to_images(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgtah5CQWgeV"
      },
      "source": [
        "We'll use a PDF about climate change and write queries related to this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8cbXMhRUliA",
        "outputId": "8c7ce0f5-80b5-41ca-cd7d-dba7c49775cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF saved to document.pdf\n"
          ]
        }
      ],
      "source": [
        "pdf_url = \"https://climate.ec.europa.eu/system/files/2018-06/youth_magazine_en.pdf\"\n",
        "pdf_path = download_pdf(pdf_url)\n",
        "document_images = pdf_to_images(pdf_path)\n",
        "\n",
        "document_images = document_images[4:10]\n",
        "\n",
        "queries = [\n",
        "    {\"text\": \"How much did the world temperature change so far?\"},\n",
        "    {\"text\": \"What are the main causes of climate change?\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DilxeuU8hITd"
      },
      "source": [
        "We can now embed the documents and queries and get similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRsaxlTZUOGs",
        "outputId": "b501bfa7-9319-4ce3-a2ae-5025faa9e74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query embeddings shape: torch.Size([2, 2048])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "from scripts.qwen3_vl_embedding import Qwen3VLEmbedder\n",
        "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
        "\n",
        "embedder = Qwen3VLEmbedder(\"Qwen/Qwen3-VL-Embedding-2B\")\n",
        "\n",
        "document_inputs = []\n",
        "for idx, img in enumerate(document_images):\n",
        "    img_path = f\"temp_page_{idx}.png\"\n",
        "    img.save(img_path)\n",
        "    document_inputs.append({\"image\": img_path})\n",
        "\n",
        "document_embeddings = embedder.process(document_inputs)\n",
        "query_embeddings = embedder.process(queries)\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P852Uf7ehMmF"
      },
      "source": [
        "Now we will get the similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgqYF3mWU1G4",
        "outputId": "a7783806-0044-4120-c471-e6cf32ae5a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query 1: How much did the world temperature change so far?\n",
            "------------------------------------------------------------\n",
            "Top 3 pages (by similarity):\n",
            "  1. Page 1 (score: 0.4605)\n",
            "  2. Page 5 (score: 0.4408)\n",
            "  3. Page 2 (score: 0.4360)\n",
            "\n",
            "Query 2: What are the main causes of climate change?\n",
            "------------------------------------------------------------\n",
            "Top 3 pages (by similarity):\n",
            "  1. Page 1 (score: 0.4903)\n",
            "  2. Page 2 (score: 0.4770)\n",
            "  3. Page 5 (score: 0.4555)\n"
          ]
        }
      ],
      "source": [
        "def retrieve_top_k(query_embedding, document_embeddings, k=3):\n",
        "    \"\"\"Retrieve top-k most similar documents for a query\"\"\"\n",
        "    if torch.is_tensor(query_embedding):\n",
        "      query_embedding = query_embedding.cpu().numpy()\n",
        "    if torch.is_tensor(document_embeddings):\n",
        "      document_embeddings = document_embeddings.cpu().numpy()\n",
        "    similarity_scores = query_embedding @ document_embeddings.T\n",
        "    top_k_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
        "    top_k_scores = similarity_scores[top_k_indices]\n",
        "    return top_k_indices, top_k_scores\n",
        "\n",
        "\n",
        "for query_idx, query in enumerate(queries):\n",
        "    print(f\"\\nQuery {query_idx + 1}: {query['text']}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    top_indices, top_scores = retrieve_top_k(\n",
        "        query_embeddings[query_idx],\n",
        "        document_embeddings,\n",
        "        k=3\n",
        "    )\n",
        "\n",
        "    print(f\"Top 3 pages (by similarity):\")\n",
        "    for rank, (page_idx, score) in enumerate(zip(top_indices, top_scores), 1):\n",
        "        print(f\"  {rank}. Page {page_idx + 1} (score: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ukF0iGUhTuF"
      },
      "source": [
        "We can remove embedder to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18H6qH5HhewE"
      },
      "outputs": [],
      "source": [
        "del embedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PPYyAzshWsg"
      },
      "source": [
        "Let's rerank results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcZwFxBXaOpi",
        "outputId": "2bb9db6b-722a-4fdd-d5c4-2b964f641989"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reranking results for: How much did the world temperature change so far?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2914: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1. Page 1 (reranker score: 0.5188)\n",
            "  2. Page 5 (reranker score: 0.4743)\n",
            "  3. Page 2 (reranker score: 0.6626)\n",
            "\n",
            "Best page after reranking: Page 2\n"
          ]
        }
      ],
      "source": [
        "from scripts.qwen3_vl_reranker import Qwen3VLReranker\n",
        "\n",
        "reranker = Qwen3VLReranker(\"Qwen/Qwen3-VL-Reranker-2B\")\n",
        "\n",
        "# Enable FA2 if your GPU allows\n",
        "# reranker = Qwen3VLReranker(\n",
        "#     model_name_or_path=reranker_model_name,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     attn_implementation=\"flash_attention_2\"\n",
        "# )\n",
        "\n",
        "query_for_reranking = queries[0]\n",
        "top_indices, _ = retrieve_top_k(query_embeddings[0], document_embeddings, k=3)\n",
        "\n",
        "print(f\"\\nReranking results for: {query_for_reranking['text']}\")\n",
        "\n",
        "reranker_inputs = {\n",
        "    \"instruction\": \"Retrieve pages relevant to the user's query about climate change.\",\n",
        "    \"query\": query_for_reranking,\n",
        "    \"documents\": [{\"image\": f\"temp_page_{idx}.png\"} for idx in top_indices],\n",
        "    \"fps\": 1.0\n",
        "}\n",
        "\n",
        "reranker_scores = reranker.process(reranker_inputs)\n",
        "\n",
        "for rank, (page_idx, score) in enumerate(zip(top_indices, reranker_scores), 1):\n",
        "    print(f\"  {rank}. Page {page_idx + 1} (reranker score: {score:.4f})\")\n",
        "\n",
        "best_page_idx = top_indices[np.argmax(reranker_scores)]\n",
        "print(f\"\\nBest page after reranking: Page {best_page_idx + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dNf6C_uhErk"
      },
      "source": [
        "We are done with reranker, we can remove it to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o86242ZmhDBV"
      },
      "outputs": [],
      "source": [
        "del reranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHsnSj1hhaQ"
      },
      "source": [
        "We will now initialize the Qwen3-VL model, pass our best ranked page as well as our text prompt to let Qwen3-VL generate the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYPnnSgRVlya",
        "outputId": "f715dfb7-be64-4bfc-b80a-ea735dbde855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating answer for: How much did the world temperature change so far?\n",
            "Using retrieved page: 2\n",
            "\n",
            "Query: How much did the world temperature change so far?\n",
            "\n",
            "Answer:\n",
            "Based on the information provided in the document, the world temperature has changed by approximately 1.1°C since the late 19th century.\n"
          ]
        }
      ],
      "source": [
        "vlm_model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen3-VL-2B-Instruct\",\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ").to(\"cuda\")\n",
        "# Enable FA2 for better performance\n",
        "# vlm_model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "#     \"Qwen/Qwen3-VL-2B-Instruct\",\n",
        "#     dtype=torch.bfloat16,\n",
        "#     attn_implementation=\"flash_attention_2\",\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Instruct\")\n",
        "\n",
        "print(f\"Generating answer for: {query_for_reranking['text']}\")\n",
        "print(f\"Using retrieved page: {best_page_idx + 1}\")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"image\": f\"temp_page_{best_page_idx}.png\",\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"Based on this page from a climate change document, please answer the following question: {query_for_reranking['text']}\"\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "inputs = inputs.to(vlm_model.device)\n",
        "\n",
        "generated_ids = vlm_model.generate(**inputs, max_new_tokens=256)\n",
        "generated_ids_trimmed = [\n",
        "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "output_text = processor.batch_decode(\n",
        "    generated_ids_trimmed,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(f\"\\nQuery: {query_for_reranking['text']}\")\n",
        "print(f\"\\nAnswer:\\n{output_text[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMk8V_-veJPY"
      },
      "source": [
        "The answer is correct!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
